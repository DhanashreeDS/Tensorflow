{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9daec89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# only for cuda enabled laptop and desktop\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "if physical_devices:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3996b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb5e128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06ee2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cc3d829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
       "         4.9800e+00],\n",
       "        [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
       "         9.1400e+00],\n",
       "        [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
       "         4.0300e+00],\n",
       "        ...,\n",
       "        [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         5.6400e+00],\n",
       "        [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "         6.4800e+00],\n",
       "        [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         7.8800e+00]]),\n",
       " 'target': array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
       "        18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
       "        15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
       "        13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
       "        21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
       "        35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
       "        19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
       "        20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
       "        23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
       "        33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
       "        21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
       "        20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
       "        23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
       "        15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
       "        17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
       "        25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
       "        23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
       "        32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
       "        34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
       "        20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
       "        26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
       "        31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
       "        22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
       "        42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
       "        36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
       "        32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
       "        20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
       "        20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
       "        22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
       "        21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
       "        19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
       "        32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
       "        18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
       "        16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
       "        13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
       "         7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
       "        12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
       "        27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
       "         8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
       "         9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
       "        10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
       "        15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
       "        19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
       "        29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
       "        20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
       "        23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9]),\n",
       " 'feature_names': array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "        'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7'),\n",
       " 'DESCR': \".. _boston_dataset:\\n\\nBoston house prices dataset\\n---------------------------\\n\\n**Data Set Characteristics:**  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n.. topic:: References\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n\",\n",
       " 'filename': 'C:\\\\Users\\\\sahar\\\\.conda\\\\envs\\\\deepl\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\boston_house_prices.csv'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61f4a206",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(data.data,columns=data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c703fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['price'] = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deae87b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
       "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
       "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
       "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
       "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
       "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  price  \n",
       "0       15.3  396.90   4.98   24.0  \n",
       "1       17.8  396.90   9.14   21.6  \n",
       "2       17.8  392.83   4.03   34.7  \n",
       "3       18.7  394.63   2.94   33.4  \n",
       "4       18.7  396.90   5.33   36.2  \n",
       "..       ...     ...    ...    ...  \n",
       "501     21.0  391.99   9.67   22.4  \n",
       "502     21.0  396.90   9.08   20.6  \n",
       "503     21.0  396.90   5.64   23.9  \n",
       "504     21.0  393.45   6.48   22.0  \n",
       "505     21.0  396.90   7.88   11.9  \n",
       "\n",
       "[506 rows x 14 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19198b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:,0:13].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91f3fda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
       "        4.9800e+00],\n",
       "       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
       "        9.1400e+00],\n",
       "       [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
       "        4.0300e+00],\n",
       "       ...,\n",
       "       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        5.6400e+00],\n",
       "       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "        6.4800e+00],\n",
       "       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        7.8800e+00]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26cd2631",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset.iloc[:,13].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b11e6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
       "       18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
       "       15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
       "       13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
       "       21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
       "       35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
       "       19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
       "       20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
       "       23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
       "       33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
       "       21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
       "       20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
       "       23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
       "       15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
       "       17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
       "       25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
       "       23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
       "       32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
       "       34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
       "       20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
       "       26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
       "       31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
       "       22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
       "       42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
       "       36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
       "       32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
       "       20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
       "       20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
       "       22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
       "       21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
       "       19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
       "       32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
       "       18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
       "       16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
       "       13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
       "        7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
       "       12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
       "       27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
       "        8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
       "        9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
       "       10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
       "       15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
       "       19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
       "       29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
       "       20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
       "       23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ace4f128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07d0ddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c111729",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecb9168e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.41978194,  0.28482986, -1.2879095 , ..., -1.45900038,\n",
       "         0.44105193, -1.0755623 ],\n",
       "       [-0.41733926, -0.48772236, -0.59338101, ..., -0.30309415,\n",
       "         0.44105193, -0.49243937],\n",
       "       [-0.41734159, -0.48772236, -0.59338101, ..., -0.30309415,\n",
       "         0.39642699, -1.2087274 ],\n",
       "       ...,\n",
       "       [-0.41344658, -0.48772236,  0.11573841, ...,  1.17646583,\n",
       "         0.44105193, -0.98304761],\n",
       "       [-0.40776407, -0.48772236,  0.11573841, ...,  1.17646583,\n",
       "         0.4032249 , -0.86530163],\n",
       "       [-0.41500016, -0.48772236,  0.11573841, ...,  1.17646583,\n",
       "         0.44105193, -0.66905833]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "614db47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b489d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e4c9404",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f937a345",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(100,activation='relu',input_dim=13))\n",
    "model.add(Dense(50,activation='relu'))\n",
    "model.add(Dense(20,activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f811e12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 100)               1400      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                1020      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 7,491\n",
      "Trainable params: 7,491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ffbe24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d4babd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "16/16 [==============================] - 3s 5ms/step - loss: 580.2297\n",
      "Epoch 2/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 514.6373\n",
      "Epoch 3/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 386.8235\n",
      "Epoch 4/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 189.0952\n",
      "Epoch 5/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 71.2486\n",
      "Epoch 6/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 47.6875\n",
      "Epoch 7/1500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 32.0598\n",
      "Epoch 8/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 26.2209\n",
      "Epoch 9/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 23.1369\n",
      "Epoch 10/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 20.9316\n",
      "Epoch 11/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 19.4137\n",
      "Epoch 12/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 18.2117\n",
      "Epoch 13/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 17.1035\n",
      "Epoch 14/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 16.3065\n",
      "Epoch 15/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 15.6106\n",
      "Epoch 16/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 15.0518\n",
      "Epoch 17/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 14.4587\n",
      "Epoch 18/1500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 13.8811\n",
      "Epoch 19/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 13.3992\n",
      "Epoch 20/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.9632\n",
      "Epoch 21/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.5600\n",
      "Epoch 22/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.3096\n",
      "Epoch 23/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.9162\n",
      "Epoch 24/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.6148\n",
      "Epoch 25/1500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 11.5511\n",
      "Epoch 26/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.0746\n",
      "Epoch 27/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 10.9773\n",
      "Epoch 28/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 10.6907\n",
      "Epoch 29/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 10.6768\n",
      "Epoch 30/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 10.4159\n",
      "Epoch 31/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 10.2145\n",
      "Epoch 32/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 9.9557\n",
      "Epoch 33/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 9.9131\n",
      "Epoch 34/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 9.8471\n",
      "Epoch 35/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 9.4942\n",
      "Epoch 36/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 9.3029\n",
      "Epoch 37/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 9.1625\n",
      "Epoch 38/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 8.9256\n",
      "Epoch 39/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8.8832\n",
      "Epoch 40/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 8.7375\n",
      "Epoch 41/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 8.5885\n",
      "Epoch 42/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 8.5761\n",
      "Epoch 43/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 8.4127\n",
      "Epoch 44/1500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 8.1371\n",
      "Epoch 45/1500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.9691\n",
      "Epoch 46/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 8.0158\n",
      "Epoch 47/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8365\n",
      "Epoch 48/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.6545\n",
      "Epoch 49/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.6305\n",
      "Epoch 50/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.3478\n",
      "Epoch 51/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.1950\n",
      "Epoch 52/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.2675\n",
      "Epoch 53/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.1781\n",
      "Epoch 54/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.9696\n",
      "Epoch 55/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6.9272\n",
      "Epoch 56/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6.7713\n",
      "Epoch 57/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.6638\n",
      "Epoch 58/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.7064\n",
      "Epoch 59/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4859\n",
      "Epoch 60/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4190\n",
      "Epoch 61/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3660\n",
      "Epoch 62/1500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.2023\n",
      "Epoch 63/1500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.3561\n",
      "Epoch 64/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.0787\n",
      "Epoch 65/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.0937\n",
      "Epoch 66/1500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.1697\n",
      "Epoch 67/1500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9512\n",
      "Epoch 68/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.0702\n",
      "Epoch 69/1500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7316\n",
      "Epoch 70/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.2200\n",
      "Epoch 71/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.7556\n",
      "Epoch 72/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3924\n",
      "Epoch 73/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.5799\n",
      "Epoch 74/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.5533\n",
      "Epoch 75/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.4269\n",
      "Epoch 76/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.5214\n",
      "Epoch 77/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.3600\n",
      "Epoch 78/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.1770\n",
      "Epoch 79/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2575\n",
      "Epoch 80/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.1582\n",
      "Epoch 81/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.1760\n",
      "Epoch 82/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.0815\n",
      "Epoch 83/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.2407\n",
      "Epoch 84/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.0623\n",
      "Epoch 85/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.9163\n",
      "Epoch 86/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8797\n",
      "Epoch 87/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8195\n",
      "Epoch 88/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.7892\n",
      "Epoch 89/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0936\n",
      "Epoch 90/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2793\n",
      "Epoch 91/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.0200\n",
      "Epoch 92/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7146\n",
      "Epoch 93/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.5816\n",
      "Epoch 94/1500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6012\n",
      "Epoch 95/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.5586\n",
      "Epoch 96/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.5035\n",
      "Epoch 97/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3660\n",
      "Epoch 98/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3604\n",
      "Epoch 99/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3573\n",
      "Epoch 100/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4867\n",
      "Epoch 101/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.5488\n",
      "Epoch 102/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.2988\n",
      "Epoch 103/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3837\n",
      "Epoch 104/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.4298\n",
      "Epoch 105/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.0716\n",
      "Epoch 106/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.1380\n",
      "Epoch 107/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.1245\n",
      "Epoch 108/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.1162\n",
      "Epoch 109/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.1164\n",
      "Epoch 110/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.9243\n",
      "Epoch 111/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.9325\n",
      "Epoch 112/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.9839\n",
      "Epoch 113/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.8217\n",
      "Epoch 114/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.8350\n",
      "Epoch 115/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.8008\n",
      "Epoch 116/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.8153\n",
      "Epoch 117/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.6374\n",
      "Epoch 118/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.6093\n",
      "Epoch 119/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.6420\n",
      "Epoch 120/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.5758\n",
      "Epoch 121/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.6014\n",
      "Epoch 122/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7654\n",
      "Epoch 123/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.6151\n",
      "Epoch 124/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.5305\n",
      "Epoch 125/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.4482\n",
      "Epoch 126/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.4529\n",
      "Epoch 127/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.4584\n",
      "Epoch 128/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.4938\n",
      "Epoch 129/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.6548\n",
      "Epoch 130/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.6943\n",
      "Epoch 131/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.5302\n",
      "Epoch 132/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.3378\n",
      "Epoch 133/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.1666\n",
      "Epoch 134/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.2221\n",
      "Epoch 135/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.2665\n",
      "Epoch 136/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.3045\n",
      "Epoch 137/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.3019\n",
      "Epoch 138/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.2649\n",
      "Epoch 139/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.2972\n",
      "Epoch 140/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.2618\n",
      "Epoch 141/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.3609\n",
      "Epoch 142/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.0037\n",
      "Epoch 143/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2.9935\n",
      "Epoch 144/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2.9878\n",
      "Epoch 145/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.0394\n",
      "Epoch 146/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2.9694\n",
      "Epoch 147/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.0219\n",
      "Epoch 148/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.9150\n",
      "Epoch 149/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.9172\n",
      "Epoch 150/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.8717\n",
      "Epoch 151/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.9037\n",
      "Epoch 152/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.9818\n",
      "Epoch 153/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2.9021\n",
      "Epoch 154/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.7605\n",
      "Epoch 155/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.7661\n",
      "Epoch 156/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.9262\n",
      "Epoch 157/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.7129\n",
      "Epoch 158/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.7288\n",
      "Epoch 159/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.6330\n",
      "Epoch 160/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.6576\n",
      "Epoch 161/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.6768\n",
      "Epoch 162/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.6548\n",
      "Epoch 163/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.6560\n",
      "Epoch 164/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.5387\n",
      "Epoch 165/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.4958\n",
      "Epoch 166/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.6267\n",
      "Epoch 167/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2.5811\n",
      "Epoch 168/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.6412\n",
      "Epoch 169/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.7111\n",
      "Epoch 170/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2.4801\n",
      "Epoch 171/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2.4243\n",
      "Epoch 172/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2.4223\n",
      "Epoch 173/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2.3342\n",
      "Epoch 174/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.3959\n",
      "Epoch 175/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.3852\n",
      "Epoch 176/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.4937\n",
      "Epoch 177/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.6144\n",
      "Epoch 178/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.4885\n",
      "Epoch 179/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.3251\n",
      "Epoch 180/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.3168\n",
      "Epoch 181/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2.5227\n",
      "Epoch 182/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2.5244\n",
      "Epoch 183/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2.5813\n",
      "Epoch 184/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2.2703\n",
      "Epoch 185/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.1616\n",
      "Epoch 186/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.2089\n",
      "Epoch 187/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.1728\n",
      "Epoch 188/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.2966\n",
      "Epoch 189/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.1293\n",
      "Epoch 190/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2.2419\n",
      "Epoch 191/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.0660\n",
      "Epoch 192/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2.1522\n",
      "Epoch 193/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.1944\n",
      "Epoch 194/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.1829\n",
      "Epoch 195/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.0749\n",
      "Epoch 196/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.0412\n",
      "Epoch 197/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.3033\n",
      "Epoch 198/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 2.0955\n",
      "Epoch 199/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.0745\n",
      "Epoch 200/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.0198\n",
      "Epoch 201/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.0944\n",
      "Epoch 202/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.9786\n",
      "Epoch 203/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2.0455\n",
      "Epoch 204/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2.0496\n",
      "Epoch 205/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2.1669\n",
      "Epoch 206/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2.1311\n",
      "Epoch 207/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2.0717\n",
      "Epoch 208/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.0654\n",
      "Epoch 209/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.1911\n",
      "Epoch 210/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.9658\n",
      "Epoch 211/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.8171\n",
      "Epoch 212/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.8016\n",
      "Epoch 213/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.7845\n",
      "Epoch 214/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.8331\n",
      "Epoch 215/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.9070\n",
      "Epoch 216/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.0312\n",
      "Epoch 217/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.9905\n",
      "Epoch 218/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.8964\n",
      "Epoch 219/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.6797\n",
      "Epoch 220/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.7103\n",
      "Epoch 221/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.8144\n",
      "Epoch 222/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.9148\n",
      "Epoch 223/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.7237\n",
      "Epoch 224/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.7832\n",
      "Epoch 225/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.6818\n",
      "Epoch 226/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.6577\n",
      "Epoch 227/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.7856\n",
      "Epoch 228/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.7516\n",
      "Epoch 229/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.6190\n",
      "Epoch 230/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.7922\n",
      "Epoch 231/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.7874\n",
      "Epoch 232/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.5911\n",
      "Epoch 233/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.6739\n",
      "Epoch 234/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.5901\n",
      "Epoch 235/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.6413\n",
      "Epoch 236/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.5439\n",
      "Epoch 237/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.7382\n",
      "Epoch 238/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.5465\n",
      "Epoch 239/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.5273\n",
      "Epoch 240/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.5485\n",
      "Epoch 241/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.5086\n",
      "Epoch 242/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4595\n",
      "Epoch 243/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.5057\n",
      "Epoch 244/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.5425\n",
      "Epoch 245/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.6870\n",
      "Epoch 246/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.6398\n",
      "Epoch 247/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.5671\n",
      "Epoch 248/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.4623\n",
      "Epoch 249/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.5525\n",
      "Epoch 250/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4337\n",
      "Epoch 251/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4965\n",
      "Epoch 252/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.5788\n",
      "Epoch 253/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.5036\n",
      "Epoch 254/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.5019\n",
      "Epoch 255/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4140\n",
      "Epoch 256/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3457\n",
      "Epoch 257/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3466\n",
      "Epoch 258/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3842\n",
      "Epoch 259/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3192\n",
      "Epoch 260/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4309\n",
      "Epoch 261/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.5413\n",
      "Epoch 262/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4326\n",
      "Epoch 263/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4690\n",
      "Epoch 264/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4789\n",
      "Epoch 265/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4942\n",
      "Epoch 266/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3914\n",
      "Epoch 267/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3579\n",
      "Epoch 268/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3527\n",
      "Epoch 269/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4960\n",
      "Epoch 270/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4728\n",
      "Epoch 271/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3564\n",
      "Epoch 272/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3203\n",
      "Epoch 273/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3478\n",
      "Epoch 274/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2088\n",
      "Epoch 275/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2281\n",
      "Epoch 276/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2637\n",
      "Epoch 277/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3606\n",
      "Epoch 278/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4588\n",
      "Epoch 279/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2665\n",
      "Epoch 280/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3055\n",
      "Epoch 281/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.6774\n",
      "Epoch 282/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4702\n",
      "Epoch 283/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2035\n",
      "Epoch 284/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1632\n",
      "Epoch 285/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.1838\n",
      "Epoch 286/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.4005\n",
      "Epoch 287/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.1701\n",
      "Epoch 288/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.3193\n",
      "Epoch 289/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.2872\n",
      "Epoch 290/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1694\n",
      "Epoch 291/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.2203\n",
      "Epoch 292/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1470\n",
      "Epoch 293/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2326\n",
      "Epoch 294/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.2087\n",
      "Epoch 295/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1561\n",
      "Epoch 296/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2410\n",
      "Epoch 297/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3101\n",
      "Epoch 298/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2276\n",
      "Epoch 299/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1098\n",
      "Epoch 300/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0692\n",
      "Epoch 301/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.1316\n",
      "Epoch 302/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1443\n",
      "Epoch 303/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1343\n",
      "Epoch 304/1500\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.0380\n",
      "Epoch 305/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0640\n",
      "Epoch 306/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0436\n",
      "Epoch 307/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1047\n",
      "Epoch 308/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0966\n",
      "Epoch 309/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1051\n",
      "Epoch 310/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1665\n",
      "Epoch 311/1500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0490\n"
     ]
    }
   ],
   "source": [
    "model.fit(X,y,epochs=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fc0189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
